1.什么是字符集
	字符集只是一个规则集合的名字，对应到真实生活中，字符集就是对某种语言的称呼。
	如：英语，汉语，日语。

	对于一个字符集来说要正确编码转码一个字 符需要三个关键元素：
		字库表（character repertoire）、
		编码字符集（coded character set）、
		字符编码（character encoding form）。
        两种不同之间的编码的转换：应该是把两种字库表之间建立一个映射关系。映射的关系是吧两种字库表之间 相同的字符的二进制建立映射关系。（就是把一个字符，在两种字库表之间的所表示的二进制的等于起来);
            例如：在UTF-8里面"我"==001212,在GBK里面"我"==2255，将001212==2255起来就可以互相转换了。如果没有响应的对应关系，就是一方字库表里面没有相同的字符，那么只能用缺失字符来补位了！

字库表（character repertoire）：
	其中字库表是一个相当于所有可读或者可显示字符的数据库，字库表决定了整个字符集能够展现表示的所有字符的范围。
	
编码字符集（coded character set）：
	即用一个编码值 code point来表示一个字符在字库中的位置。

字符编码（character encoding form）：
	将编码字符集和实际存储数值之间的转换关系。一般来说都会直接将code point的值作为编码后的值直接存储。
		例如在ASCII中A在表中排第65位，而编码后A的数值是0100 0001也即十进制的65的二进制转换结果。
		但是
		原因：
			统一字库表的目的是为了能够涵盖世界上所有的字符，但实际使用过程中会发现真正用的上的字符相对整个字库表来说比例非常 低。
			例如中文地区的程序几乎不会需要日语字符，而一些英语国家甚至简单的ASCII字库表就能满足基本需求。
			而如果把每个字符都用字库表中的序号来存储的 话，每个字符就需要3个字节（这里以Unicode字库为例），
			这样对于原本用仅占一个字符的ASCII编码的英语地区国家显然是一个额外成本（存储体积 是原来的三倍）。
			算的直接一些，同样一块硬盘，用ASCII可以存1500篇文章，而用3字节Unicode序号存储只能存500篇。
			于是就出现了 UTF-8这样的变长编码。在UTF-8编码中原本只需要一个字节的ASCII字符，仍然只占一个字节。
			而像中文及日语这样的复杂字符就需要2个到3个字 节来存储。
		
	注意：	编码字符集Unicode，有UTF-8、UTF-16、UTF-32等多种字符编码
		编码字符集ASCII,本身就是编码字符集，又是字符编码
		编码字符集CB2312，只有EUC-CN一种字符编码


	编码字符集是Unicode，在内存中的形式，
	字符编码是UTF-8，存储到硬盘里面的形式。
		
因为GBK和UTF-8编码字符集是不同的，GBK有自己的编码字符集，UTF-8则是使用Unicode的编码字符集;


*****GBK 转成 Unicode 内码（如果要存储内码流可能会是UTF-16，如果一个字符一个字符的转就存成 uint32就好），
	内码再转成 UTF-8。每一种编码和Unicode之间都有双向的查找表。
	***Windows就喜欢在内存里处理UTF16，当然不排除以后会升级成UTF-32

	因为ISO-8859-1编码范围使用了单字节内的所有空间，在支持ISO-8859-1的系统中传输和存储其他任何编码的字节流都不会被抛弃。
	换言之，把其他任何编码的字节流当作ISO-8859-1编码看待都没有问题。这是个很重要的特性，MySQL数据库默认编码是Latin1就是利用了这个特性。
	ASCII编码是一个7位的容器，ISO-8859-1编码是一个8位的容器。


	因为通过那个帖子关于 UTF-8 的编码规则表，先看第一个字节 D6，二进制为 11010110，因此可用的 UTF-8
	采用第二行的规则，即：110xxxxx 10xxxxxx，根据这一行规则，要求第二个字节二进制是以 10 开头的，
	但是我们第二个字节 D0 的二进制为 11010000，因此不符合 UTF-8 的编码规则，因此将采用 UTF-8 的
	缺失字符 EF BF BD 将其进行编码，因为 D6 D0 这两个字节 UTF-8 都不认识，因此被 UTF-8 编码成为
	EF BF BD EF BF BD 共 6 个字节的两个缺失字符。

	某编码字库表里面没有与之字节对应的字，所以采用了缺失字符。这样就导致了数据丢失



    byte[] b1 = name.getBytes("gbk");
		//用iso-8859-1字符集解码b1里面的字节，将其变成我们看得懂的字符串
		String bytes2 = new String(b1,"iso-8859-1");
		System.out.println(bytes2);
		//
		String bytes3 = new String(bytes2.getBytes("iso-8859-1"),"gbk");
		System.out.println(bytes3);
		/*
		 * 1.是将字节Unicode编码转换成gbk编码的字节
		 * 2.将gbk的编码的字节（二进制数）用ISO-8859-1解码成我们看的懂得字符
		 * 3.因为编码的时候字节是用GBK的字符集编码的，不是用用ISO-8859-1映射的
		 * 4.所以显示的是乱码，因为ISO-8859-1对每个字节都有相应的映射,没有未知字节映射，
		 * 	各个编码如果所属的字符集里面没有字节相应的字符的映射的话，就会使用缺失字符映射，
		 * 5.然后使用ISO-8859-1对ISO-8859-1解码后的字符再次进行编码。这时就把字符转换成字节了
		 *	因为没有缺失字节，所以还是之前在使用ISO-8859-1解码前的字节，因为这个字节是
		 *	使用Utf-8编码的，所以你再使用Utf-8解码，然后就解码成相应的字符“张三”了
		 * */


自己理解的Unicode与各个编码之间的转换
         在Windows里面内存里面的编码是Unicode(utf-16)编码的，然后保存时会将其二次编码，以转变成GBK，UTF-8等编码！
         这之间转换是要通过两个编码的字库表之间的映射的，
         1.先用Unicode字符集查询Unicode的字库表中所表示的字符，然后在GBK的字库表查询这个字符，得到GBK字库表里面的code point,然后将它存储！
        
char在java里面的中文字符使用的编码
         char使用2字节的unicode编码，兼容ascii码，2字节表示的整数范围是0~65535，1字节表示的是0~255，
         0~255表示的字符在大多数字符集里都与ascii码保持一致，所以unicode编码的字符C和ASCII编码的字符C对应的数字是相同的

客户端的信息（UTF-8） --> 服务器（没有设置过编码，默认使用iso-8859-1） --> 把iso-8859-1的字符对应映射表转换成Unicode编码放在内存里面 --> 
   -->根据浏览器的请求头的编码将其Unicode转换成相应的编码 --> 将其打印回浏览器 --> 乱码
    JAVA使用UNICODE来存储字符数据，处理字符时通常有三个步骤：

    1、按指定的字符编码形式，从源输入流中读取字符数据

    2、以UNICODE编码形式将字符数据存储在内存中

    3、按指定的字符编码形式，将字符数据编码并写入目的输出流中

    所以JAVA处理字符时总是经过了两次编码转换，一次是从指定编码转换为UNICODE编码，一次是从UNICODE编码转换为指定编码。
    如果在读入时用错误的形式解码字符，则内存存储的是错误的UNICODE字符。而从最初文件中读出的字符数据，到最终在屏幕终端显示这些字符，期间经过了应用程序的多次转换。
    如果中间某次字符处理用错误的编码方式解码了从输入流读取的字符数据，或用错误的编码方式将字符写入输出流，则下一个字符数据的接收者就会编解码出错，从而导致最终显示乱码。
    这一点，是我们分析字符编码问题以及解决问题的指导思想。


get请求：首先说下客户端（浏览器）的form表单用get方法是如何将数据编码后提交给服务器的把
    
    对于get方法来说，都是把数据串联在请求的url后面作为参数，如：http://localhost:8080/servlet?msg=abc （很常见的一个乱码问题就要出现了，如果url中出现中文或其它特殊字符的话，
        如：http://localhost:8080/servlet?msg=杭州，服务器端容易得到乱码）， url拼接完成后，浏览器会对url进行URL encode，然后发送给服务器，url encode
        的过程就是把部分url作为字符，按照某种编码方式（如：utf-8,gbk等）编码成二进制的字节码，然后每个字节用一个包含3个字符的字符串 "%xy" 表示，其中xy为该字节的两位十六进制表示形式。
        我这里可能说不清楚，具体介绍可以看java.net.URLEncoder类的介绍。了解了URL encode的过程，我们能看见两个很重要的问题，
        第一：需要URL encode的字符都是非ASCII的字符（笼统的来讲），再通俗的来讲就是除了英文字母以外的文字（如：日文、中文等）都要进行URL encode，
        所以对于我们来说，都是英文字母的url不会出现服务器得到乱码的问题（因为不需要进行url encode，直接传输的），出现乱码的都是URL里面的带了中文或特殊字符造成的；
        第二：URL encode到底按照哪种编码方式对字符编码：这里就是浏览器的事情了，而且不同的浏览器有不同的做法，中文版的浏览器一般会默认的使用GBK，通过设置浏览器也可以使用UTF-8，
        可能不同的用户就有不同的浏览器设置，也就造成不同的编码方式，
            所以很多网站的做法都是先把url里面的中文或特殊字符用javascript做URL encode，然后在拼接url提交数据（这样就可以统一编码了，因为URL encode的时候，用的是一样的编码）
            也就是替浏览器做了URL encode，好处就是网站可以统一get方法提交数据的编码方式。完成了URL encode，那么现在url就成了ASCII范围内的字符了，
            然后以ISO-8859-1的编码方式转换成二进制随着请求头一起发送出去。这里想多说几句的是，对于get方法来说，没有请求实体，含有数据的url都在请求头里面，
            之所以用URL encode，我个人觉得原因是：对于请求头来说最终都是要用iso-8859-1编码方式编码成二进制的1010...的纯数据在互联网上传送，如果直接
            将含有中文等特殊字符做ISO-8859-1编码会全丢失数据，所以先做URL encode是有必要的。 

服务端（tomcat）是如何将数据获取到进行解码的
    第一步：是先把数据用iso-8859-1进行解码，对于get方法来说，tomcat获取数据的是ASCII范围内的请求头字符，其中的请求url里面带有参数数据，如果参
    数中有中文等特殊字符，那么目前还是URL encode后的%XY状态，先停下，我们先说下开发人员一般获取数据的过程。通常大家都是request.getParameter("name")获取参数数据，
    我们在request对象或得的数据都是经过解码过的，而解码过程中程序里是无法指定，这里要说下，有很多新手说用request.setCharacterEncoding("字符集")可以指定解码方式，
    其实是不可以的，看servlet的官方API说明有对此方法的解释：
    覆盖该请求主体中使用的字符编码的名称。在读取请求参数或使用getReader()读取输入之前，必须调用此方法。
    可以看出对于get方法他是无能为力的。那么到底用什么编码方式解码数据的呢，这是tomcat的事情了，默认缺省用的是iso-8859-1,这样我们就能找到为什么
    get请求带中文参数为什么在服务器端得到乱码了，原因是在客户端一般都是用UTF-8或GBK对数据URL encode，这里用iso-8859-1方式URL decoder显然不行，
    在程序里我们可以直接 ew String(request.getParameter("name").getBytes("iso-8859-1"),"客户端指定的URL encode编码方式")还原回字节码，
    然后用正确的方式解码数据，网上的文章通常是在tomcat里面做个配置<</span>Connector port="8080" protocol="HTTP/1.1" maxThreads="150" connectionTimeout="20000" redirectPort="8443" URIEncoding="GBK"/>    
    这样是让tomcat在获取数据后用指定的方式URL decoder，URL decoder的介绍在这里；


post提交
    1.客户端（浏览器）的form表单用post方法是如何将数据编码后提交给服务器端的。 
      在post方法里所要传送的数据也要URL encode，那么他是用什么编码方式的呢？ 
        在form所在的html文件里如果有段代码，那么post就会用此处指定的编码方式编码。一般大家都认为这段代码是为了让浏览器知道用什么字符集来对网页解释，所以网站都会把它放在html代码的最前端，尽量不出现乱码，
        其实它还有个作用就是指定form表单的post方法提交数据的URL encode编码方式。从这里可以看出对于get方法来数，浏览器对数据的URL encode的编码方式是有浏览器设置来决定，（可以用js做统一指定），
        而post方法，开发人员可以指定。 

        2。服务器端（tomcat）是如何将数据获取到进行解码的。 
        如果用tomcat默认缺省设置，也没做过滤器等编码设置，那么他也是用iso-8859-1解码的，但是request.setCharacterEncoding("字符集")可以派上用场。 
        我发现上面说的tomcat所做的事情前提都是在请求头里没有指定编码方式，如果请求头里指定了编码方式将按照这种方式编码。

首先要了解JAVA处理字符的原理。
    java使用Uuicode来存储字符数据，处理字符时通常有三个步骤“
    1.按指定的字符编码形式，从源输入流中读取数据
    2.以Unicode编码形式将字符数据存储在内存中
    3.按指定的字符编码形式，将字符数据编码并写入目的的输出流中
    的、所以JAVA处理字符时总是经过了两次编码转换，一次是从指定编码转换为unicode编码，一次是从unicode编码转换为指定编码。
    如果在读入时用错误的形式解码字符，则内存存储的是错误的Unicode字符。而从最初文件中读出的字符数据，到最终在屏幕终端显示这些字符
    期间经过了应用程序的多次转换。如果中间某次字符处理用错误的编码方式解码了从输入流读取的字符数据，或用错误的编码方式将字符写入输出流，
    则下一个字符数据的接收者就会编解码出错，从而导致最终显示乱码。这一点，是我们分析字符编码问题以及解决问题的指导思想。

    在使用iso8859-1编码的时候，如果编码UTF-8的中文（或其他非ASCII嘛的编码），因为iso8859-1字库表里面没有中文，所以无法编码，但是
    如果将其已经编码好的编码UTF-8（或其他编码）再次编码的话就没事，因为iso8859-1编码是单字节编码，会将其编码后的二进制码表分来
    一个个存储，因为iso8859-1的单字节已经全部利用，所以不会出现缺失字符。。

        在java中就会乱码

        



  
查询字符串 queryString 指的是URL?后面的参数
    在地址栏直接输入的URL后的参数是按照系统默认编码的，因为系统默认编码是GBK（因为是中文系统）
     
     get和POST请求

    request.setCharacterEncoding函数只对POST的参数有效，对GET的参数无效。且这个函数必须是在第一次调用 request.getParameter之前使用。
    这是因为Parameters类有两个字符编码参数， 一个是encoding，另一个是 queryStringEncoding，而setCharacterEncoding设置的是encoding，
    这个是在解析POST的参数是才用到的。 
    在请求一个页面时，TOMCAT会尝试构造一个Request对象，在这个对象里，会从 Server.xml里读取URIEncoding的值，并赋值给Parameters类的queryStringEncoding变量，
    而这个变量将在 解析request.getParameter中的GET参数时用来指导字符解码。

     在调查TOMCAT的代码时发现了另一个在server.xml里的参数useBodyEncodingForURI，可以解决这个问题。这个参数设成 true后，
     TOMCAT就会用request.setCharacterEncoding所设置的字符编码来同样解析GET参数了。这样，那个 SetCharacterEncodingFilter就可以同时处理GET和POST参数了。


定长编码便于计算机处理，Unicode是定长编码，GBK等不是，就是每个字符表示的长度固定

所有输出在控制台上的字符都是GBK的，在JAVA内存中最后输出会将内存中的Unicode转换成GBK从而打印在控制台上！
    使用流读取文件的时候也是，根据系统默认的编码来指定流读取的文件用什么来解码
        其实就是先指定一个编码，将其二进制认定为是由此编码来编码而得到的二进制数，然后将其编码与Unicode的映射表互相映射，
            我们指定的所谓这个文件是由这个某种编码编码成的二进制数 --> 然后对其进行按照 这个编码和Unicode编码的 映射表  对其映射，
            映射表是按照其现实中的相同的字符来映射的，将两个现实中的相同的字符的 二进制 划上等号， 
            对应这个表将其转换成Unicode， 然后在内存中存储起来。  因为这个文件很有可能并不是由这个文件来我们指定的编码来编码的，所以在读取的时候读取到这个
            编码无法表示的二进制时，那么就会自动使用该编码的缺失字符来表示。然后将表示缺失字符二进制用映射表映射成Unicode

             然后再输出到硬盘的时候如果没有指定编码还是会按系统默认的编码（中文系统是GBK）GBK存储，但是在输入流转换
            的时候就已经丢失字符了，自然就会把缺失字符所代表的二进制 在按照GBK与Unicode码表的映射表 转换从GBK  如果有能表示的字符，那么就是GBK字符，如果没有的话
            就会使用GBK和映射表 互相规定的缺失字符表示 就是如果有不能映射的字符全部用那个二进制表示！

        缺失字符是由映射表指定的还是由编码指定的
            应该是由当前编码决定的，毕竟在转换bytes的时候只有编码参与了转换
            字符映射应该是要先通过该编码的规则，才能进行映射，因为映射表并不知道该二进制数是哪里开头哪里结束的，只有编码知道！当编码识别到无法识别的二进制时，就会
            使用缺失字符代替，然后将其缺失字符映射到Unicode码
            在字节转换成编码和其他字符转换成Unicode的时候使用的是字符的缺失字符
            在Unicode转换成其他编码的时候，如果映射表中没有相对应的字符，应该就会像返回null一样，返回一个固定的一个缺失字符
            这个不要深入！！只要知道缺失字符用来代替无法映射或无法编码解码的字符或二进制
            
             BufferedReader直接读取test.txt的字节内容并以默认方式构造字符串。分析BufferedReader的代码，我们可以看到 BufferedReader调用了FileReader的read方法，
             而FileReader又调用了FileInputStream的native 的read方法。所谓native的方法，就是操作系统底层方法。那么我们操作系统是中文系统，
             所以FileInputStream默认用GBK方式读取文件。因为我们保存test.txt用的是UTF-8，所以在这里读取文件内容使用GBK是错误的编码。

    在浏览器的URL地址栏里面的编码得看浏览器的，不同浏览器的编码不同

        所以很多网站的做法都是先把url里面的中文或特殊字符用javascript做URL encode，然后在拼接url提交数据（这样就可以统一编码了，因为URL encode的时候，用的是一样的编码）
            也就是替浏览器做了URL encode，好处就是网站可以统一get方法提交数据的编码方式。完成了URL encode，那么现在url就成了ASCII范围内的字符了，
            然后以ISO-8859-1的编码方式转换成二进制随着请求头一起发送出去。
    


Content-Disposition
 提取出来最核心的一点，filename=new String("中文文件名".getBytes("utf-8"), "ISO8859-1");
    先说为什么使用 ISO8859-1 编码，这个主要是由于http协议，http header头要求其内容必须为iso8859-1编码，所以我们最终要把其编码为 ISO8859-1 编码的字符串；
    但是前面为什么不直接使用 "中文文件名".getBytes("ISO8859-1"); 这样的代码呢？因为ISO8859-1编码的编码表中，根本就没有包含汉字字符，当然也就无法通过"中文文件名".
    getBytes("ISO8859-1");来得到正确的“中文文件名”在ISO8859-1中的编码值了，所以再通过new String()来还原就无从谈起了。 所以先通过 "中文文件名".getBytes("utf-8") 
    获取其 byte[] 字节，让其按照字节来编码，即在使用 new String("中文文件名".getBytes("utf-8"), "ISO8859-1") 将其重新组成一个字符串，传送给浏览器。

   所以上面的 header 应该是 ASCII 码表 而不是 ISO-8859-1
    



不是没有设计header请求头编码，而是规定了请求头编码必须是可读 US-ASCII 。


    至于  cookie 的转码 使用 encode 编码 和 decode 译码 来进行转码。
        
     encode 就是将字符以某种编码格式(UTF-8, GBK 等)将其转换成 二进制的字节码，然后每个字节使用包含 3 个字符串
        "%xy" 表示，其实 xy 为该字节的两位十六进制表示形式. 
    URL编码需要先指定一种字符编码，把字符串解码后，得到byte[],然后把小于0的字节+256，再转换成16进制，前面再添加一个%; 
    对于URL默认编码为UTF-8。UTF-8所有的非ASCII单词二进制数都是1开头的


    将其 cookie encode 以后然后将其存入 cookie 


就是 http://..  由于URL是采用ASCII字符集进行编码的，所以如果URL中含有非ASCII字符集中的字符，那就需要对其进行编码。
    再者，由于URL中好多字符是保留字，他们在URL中具有特殊的含义。如“&”表示参数分隔符，如果想要在URL中使用这些保留字，
    那就得对他们进行编码。
    
    所以我们要使用 encode 将其编码成 url 能识别的 字节码



















































